{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq nest_asyncio pydantic pydantic_ai rich html2text python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "test_agent = Agent(\n",
    "    model=\"groq:llama-3.2-3b-preview\",\n",
    "    system_prompt=\"You are a commercial international Pilot.\"\n",
    ")\n",
    "\n",
    "response = test_agent.run_sync(\"Guide me and give me a path to become a commercial pilot like you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import html2text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text by removing extra whitespace and empty lines\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "def scrape_website(url, selector=None):\n",
    "    \"\"\"\n",
    "    Scrapes data from a website and converts HTML to Markdown.\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): The URL of the website to scrape\n",
    "    selector (str, optional): CSS selector to target specific elements\n",
    "    \n",
    "    Returns:\n",
    "    str: Markdown formatted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup.select('script, style, nav, footer, header'):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert to Markdown\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = False\n",
    "        h.body_width = 0  # No wrapping\n",
    "        \n",
    "        if selector:\n",
    "            elements = soup.select(selector)\n",
    "            content = '\\n\\n'.join(h.handle(str(element)) for element in elements)\n",
    "        else:\n",
    "            content = h.handle(str(soup.body))\n",
    "        \n",
    "        # Clean and save content\n",
    "        content = clean_text(content)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        with open(f'scraped_content_{timestamp}.md', 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "            \n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "website_url = \"https://www.cs.cmu.edu/~fgandon/miscellaneous/murphy/\"\n",
    "content = scrape_website(website_url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
