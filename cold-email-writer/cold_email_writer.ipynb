{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq nest_asyncio pydantic pydantic_ai rich html2text python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "test_agent = Agent(\n",
    "    model=\"groq:llama-3.2-3b-preview\",\n",
    "    system_prompt=\"You are a commercial international Pilot.\"\n",
    ")\n",
    "\n",
    "response = test_agent.run_sync(\"Guide me and give me a path to become a commercial pilot like you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import html2text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text by removing extra whitespace and empty lines\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "def scrape_website(url, selector=None):\n",
    "    \"\"\"\n",
    "    Scrapes data from a website and converts HTML to Markdown.\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): The URL of the website to scrape\n",
    "    selector (str, optional): CSS selector to target specific elements\n",
    "    \n",
    "    Returns:\n",
    "    str: Markdown formatted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup.select('script, style, nav, footer, header'):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert to Markdown\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = False\n",
    "        h.body_width = 0  # No wrapping\n",
    "        \n",
    "        if selector:\n",
    "            elements = soup.select(selector)\n",
    "            content = '\\n\\n'.join(h.handle(str(element)) for element in elements)\n",
    "        else:\n",
    "            content = h.handle(str(soup.body))\n",
    "        \n",
    "        # Clean and save content\n",
    "        content = clean_text(content)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        with open(f'scraped_content_{timestamp}.md', 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "            \n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "website_url = \"https://www.cs.cmu.edu/~fgandon/miscellaneous/murphy/\"\n",
    "content = scrape_website(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import RunContext\n",
    "\n",
    "\n",
    "# Tool Input\n",
    "@dataclass\n",
    "class JobInformationFetchDeps:\n",
    "    job_post_url: str\n",
    "\n",
    "\n",
    "# Tool Output - Agent Input\n",
    "@dataclass\n",
    "class JobDescriptionAgentDependecies:\n",
    "    job_posting_information: str\n",
    "\n",
    "\n",
    "class JobDescriptionAgentResult(BaseModel):\n",
    "    role: str = Field(\n",
    "        description=\"The job title or role position being described (e.g., 'Senior Software Engineer', 'Product Manager')\"\n",
    "    )\n",
    "    company_name: str = Field(description=\"The Company which posted job\")\n",
    "    experience: str = Field(\n",
    "        description=\"Required years and type of experience for the position (e.g., '5+ years of software development')\"\n",
    "    )\n",
    "    skills: List[str] = Field(\n",
    "        description=\"List of specific technical skills, tools, or competencies required for the role (e.g., ['Python', 'AWS', 'Machine Learning'])\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Detailed overview of the job responsibilities, requirements, and expectations\"\n",
    "    )\n",
    "\n",
    "\n",
    "job_description_parser_agent = Agent(\n",
    "    model=\"groq:llama-3.2-3b-preview\",\n",
    "    deps_type=JobInformationFetchDeps,\n",
    "    result_type=JobDescriptionAgentResult,\n",
    "    system_prompt=\"\"\"You are a specialized HR assistant focused on analyzing and structuring job descriptions. Your primary responsibilities are:\n",
    "1. Use the get_job_details tool to retrieve job posting information\n",
    "2. Extract and categorize key components including:\n",
    "   - Core role/position title\n",
    "   - Required experience level\n",
    "   - Essential skills and qualifications\n",
    "   - Detailed role description and responsibilities\n",
    "Format all outputs according to the JobDescription schema. Be precise and consistent in your categorization. When analyzing skills:\n",
    "- Focus on specific technical and professional competencies\n",
    "- Separate distinct skills into individual items\n",
    "- Standardize skill names (e.g., \"Python\" not \"python programming\")\n",
    "If job details are ambiguous or incomplete, make reasonable inferences based on industry standards while maintaining accuracy.\"\"\",\n",
    ")\n",
    "\n",
    "@job_description_parser_agent.tool\n",
    "def get_job_details(\n",
    "    ctx: RunContext[JobInformationFetchDeps],\n",
    ") -> JobDescriptionAgentDependecies:\n",
    "    \"\"\"\n",
    "    Retrieves and extracts job posting information\n",
    "    \"\"\"\n",
    "    job_post_url = ctx.deps.job_post_url\n",
    "    job_posting_information = scrape_website(url=job_post_url)\n",
    "    # pprint(job_posting_information)\n",
    "    return JobDescriptionAgentDependecies(\n",
    "        job_posting_information=job_posting_information\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_agent_result = job_description_parser_agent.run_sync(\n",
    "    \"Please extract job description for the provided URL\",\n",
    "    deps=JobInformationFetchDeps(\n",
    "        job_post_url=\"https://openai.com/careers/ai-abuse-and-threat-intelligence-analyst/\"\n",
    "    ),\n",
    ")\n",
    "pprint(job_description_agent_result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColdEmailWriterAgentInput(BaseModel):\n",
    "    job_description: JobDescriptionAgentResult = Field(\n",
    "        description=\"Parsed job posting details including role, company, required experience, skills, and full description\"\n",
    "    )\n",
    "\n",
    "class ColdEmailWriterAgentResponse(BaseModel):\n",
    "    subject: str = Field(\n",
    "        description=\"A catchy and personalised subject line that captures attention and highlights key value proposition\"\n",
    "    )\n",
    "    body: str = Field(\n",
    "        description=\"Professional email body that matches job requirements with portfolio expertise, includes introduction, value proposition, relevant project examples, and call-to-action\"\n",
    "    )\n",
    "\n",
    "\n",
    "cold_email_writer_agent = Agent(\n",
    "    model='groq:llama-3.2-3b-preview',\n",
    "    deps_type=ColdEmailWriterAgentInput,\n",
    "    result_type=ColdEmailWriterAgentResponse,\n",
    "    system_prompt=\"\"\"\n",
    "You are Ria, a tech recruitment specialist at TopTal.io, reaching out to hiring managers about your firm's pre-vetted engineering talent pool. Using the provided job description:\n",
    "\n",
    "1. Analyze role requirements and highlight relevant TopTal portfolio projects\n",
    "2. Create concise, compelling subject lines highlighting available talent\n",
    "3. Write brief, impactful email body (3-4 paragraphs max) that:\n",
    "   - Opens with specific reference to company's hiring needs\n",
    "   - Showcases relevant TopTal's portfolio projects matching required tech stack\n",
    "   - Emphasizes that TopTal has pre-vetted engineers ready to interview\n",
    "   - Includes clear call-to-action to discuss available candidates\n",
    "\n",
    "Keep tone professional yet conversational. Focus on Toptal's talent pool and proven project experience.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cold_email_writer_agent.run_sync(\n",
    "    \"Write a cold email\",\n",
    "    deps=ColdEmailWriterAgentInput(\n",
    "        job_description=job_description_agent_result.data\n",
    "    )\n",
    ")\n",
    "pprint(result.data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
